{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/bert-standard/bert/optimization.py:87: The name tf.compat.v1.train.Optimizer is deprecated. Please use @@#train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import tokenization\n",
    "from bert import modeling\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import collections\n",
    "import re\n",
    "import random\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import bert_utils as squad_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "@@#logging.set_verbosity(@@#logging.ERROR)\n",
    "tf.compat.v1.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.autograph.set_verbosity(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load('sp10m.cased.bert.model')\n",
    "\n",
    "with open('sp10m.cased.bert.vocab') as fopen:\n",
    "    v = fopen.read().split('\\n')[:-1]\n",
    "v = [i.split('\\t') for i in v]\n",
    "v = {i[0]: i[1] for i in v}\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, v, sp_model):\n",
    "        self.vocab = v\n",
    "        self.sp_model = sp_model\n",
    "\n",
    "    def tokenize(self, string):\n",
    "        return encode_pieces(\n",
    "            self.sp_model, string, return_unicode = False, sample = False\n",
    "        )\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.sp_model.PieceToId(piece) for piece in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.sp_model.IdToPiece(i) for i in ids]\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(v, sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:00<00:00, 619.45it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130318"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = '/home/husein/pure-text/ms-train-2.0.json'\n",
    "train_examples = squad_utils.read_squad_examples(\n",
    "      input_file=train_file, is_training=True)\n",
    "\n",
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 1276.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11858"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = '/home/husein/pure-text/ms-dev-2.0.json'\n",
    "test_examples = squad_utils.read_squad_examples(\n",
    "      input_file=test_file, is_training=False)\n",
    "\n",
    "len(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130318/130318 [2:51:49<00:00, 12.64it/s]  \n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 384\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "\n",
    "train_features = squad_utils.convert_examples_to_features(\n",
    "      examples=train_examples,\n",
    "      tokenizer=tokenizer,\n",
    "      max_seq_length=max_seq_length,\n",
    "      doc_stride=doc_stride,\n",
    "      max_query_length=max_query_length,\n",
    "      is_training=True,\n",
    "      do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11858/11858 [15:50<00:00, 12.48it/s] \n"
     ]
    }
   ],
   "source": [
    "test_features = squad_utils.convert_examples_to_features(\n",
    "      examples=test_examples,\n",
    "      tokenizer=tokenizer,\n",
    "      max_seq_length=max_seq_length,\n",
    "      doc_stride=doc_stride,\n",
    "      max_query_length=max_query_length,\n",
    "      is_training=False,\n",
    "      do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('bert-squad-train.pkl', 'wb') as fopen:\n",
    "    pickle.dump([train_features, train_examples], fopen)\n",
    "    \n",
    "with open('bert-squad-test.pkl', 'wb') as fopen:\n",
    "    pickle.dump([test_features, test_examples], fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
